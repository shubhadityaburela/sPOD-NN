{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a219ecef",
   "metadata": {},
   "source": [
    "# Parametric non-intrusive model order reduction for synthetic data\n",
    "\n",
    "Here we describe the application of methods sPOD-NN, sPOD-I and POD-NN on a 1D syntheticaly generated data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c09eef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../sPOD/lib/')\n",
    "sys.path.append('../DL-ROM/LIB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ba80fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from synthetic_sup import synthetic_sup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c78cc59",
   "metadata": {},
   "source": [
    "## Basis reconstruction for the synthetic data\n",
    "\n",
    "In this part we : \n",
    "\n",
    "<ul>\n",
    "<li>Generate the 1D synthetic data with $M = 500$ and $N_t = 500$ with $training\\_samples$ being the number of different values of parameter considered for basis reconstruction and further network training and $testing\\_sample$ being the value of test parameter.</li>\n",
    "<li>Perform sPOD and POD on the generated data.</li>\n",
    "<li>Extract the time amplitudes according to Eq.(9) and Eq.(15) from the paper.</li>\n",
    "</ul>\n",
    "\n",
    "The inputs here include : \n",
    "<ul>\n",
    "<li>$spod\\_iter$ is the number of iterations for the sPOD algorithm.</li>\n",
    "<li>$nmodes$ are the number of singular values taken into consideration while creating the data.</li>\n",
    "<li>$plot\\_offline\\_data$ is the switch variable for the user to plot the results of the basis reconstruction.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8fd250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = synthetic_sup(training_samples=[0.1, 0.15, 0.2, 0.25, 0.3], testing_sample=[0.23], \n",
    "                   nmodes=8, spod_iter=300, plot_offline_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889026db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We collect the time amplitudes, shifts and the parameters for the training as well as the testing data.\n",
    "TA_TRAIN = df.TA_TRAIN\n",
    "SHIFTS_TRAIN = df.SHIFTS_TRAIN\n",
    "PARAMS_TRAIN = df.PARAMS_TRAIN\n",
    "TA_TEST = df.TA_TEST\n",
    "SHIFTS_TEST = df.SHIFTS_TEST\n",
    "PARAMS_TEST = df.PARAMS_TEST\n",
    "TA_POD_TRAIN = df.TA_POD_TRAIN\n",
    "TA_POD_TEST = df.TA_POD_TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d827d6ea",
   "metadata": {},
   "source": [
    "We assemble the $\\hat{A}$ matrix according to the Eq.(18) from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ae8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_train = np.concatenate((np.reshape(SHIFTS_TRAIN[0], newshape=[1, -1]), np.reshape(SHIFTS_TRAIN[1], newshape=[1, -1])), axis=0)\n",
    "shifts_test = np.concatenate((np.reshape(SHIFTS_TEST[0], newshape=[1, -1]), np.reshape(SHIFTS_TEST[1], newshape=[1, -1])), axis=0)\n",
    "\n",
    "ta_train = np.concatenate((TA_TRAIN, shifts_train), axis=0)\n",
    "ta_test = np.concatenate((TA_TEST, shifts_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27fe9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid, Nx : {}, Nt : {}\".format(df.Nx, df.Nt))\n",
    "print(\"Number of sPOD frames : {}\".format(df.NumFrames))\n",
    "print(\"Number of modes per frame : {}\".format(df.nmodes))\n",
    "print(\"Number of parameter instances : {}\".format(int(int(ta_train.shape[1]) / df.Nt)))\n",
    "print(\"Size of training matrix : {} x {}\".format(int(ta_train.shape[0]), int(ta_train.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7c1bb0",
   "metadata": {},
   "source": [
    "## Neural network training\n",
    "\n",
    "Based on the data which we obtain from the previous step we train our neural network. For the training we first define certain parameters needed for training step. The parameters are mentioned here are:\n",
    "\n",
    "<ul>\n",
    "<li>$scaling$ activates the min-max data scaling for efficient training.</li>\n",
    "<li>$full\\_order\\_model\\_dimension$ is effectively $M$ which is the total number of grid points.</li>\n",
    "<li>$reduced\\_order\\_model\\_dimension$ is $n_{\\mathrm{dof}}$ mentioned in Eq.(21) in the paper.</li>\n",
    "<li>$totalModes$ is the total number of modes.</li>\n",
    "<li>$num\\_early\\_stop$ defines the early stopping criteria for training step.</li>\n",
    "</ul>\n",
    "\n",
    "Subsequently the hyperparameters are:\n",
    "<ul>\n",
    "<li>$epochs$ sets the total number of epochs for training.</li>\n",
    "<li>$lr$ sets the learning rate for training.</li>\n",
    "<li>$loss\\_type$ is the type of loss to consider while training options being $L1$ or $MSE$.</li>\n",
    "<li>$batch\\_size$ sets the total number of minibatches for the training data to be broken down into for effective training.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sPOD = {\n",
    "        'scaling': True,  \n",
    "        'full_order_model_dimension': df.Nx,  \n",
    "        'reduced_order_model_dimension': ta_train.shape[0], \n",
    "        'totalModes': ta_train.shape[0] - df.NumFrames,  \n",
    "        'num_early_stop': 100000  \n",
    "    \n",
    "    }\n",
    "params_POD = {\n",
    "        'scaling': True,  \n",
    "        'full_order_model_dimension': df.Nx, \n",
    "        'reduced_order_model_dimension': TA_POD_TRAIN.shape[0], \n",
    "        'totalModes': TA_POD_TRAIN.shape[0],  \n",
    "        'num_early_stop': 500  \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2aa490",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from DFNN import run_model \n",
    "print(\"#################################\")\n",
    "print(\"sPOD-NN\")\n",
    "trained_model_sPOD, _, scaling_sPOD = run_model(ta_train, PARAMS_TRAIN, epochs=150000, lr=0.0025, loss_type='L1', \n",
    "                                                logs_folder='./DNN_result/synthetic/training_results_sPOD', \n",
    "                                                params=params_sPOD, batch_size=50)\n",
    "print(\"#################################\\n\")\n",
    "print(\"#################################\")\n",
    "print(\"POD-NN\")\n",
    "trained_model_POD, _, scaling_POD = run_model(TA_POD_TRAIN, PARAMS_TRAIN, epochs=150000, lr=0.0025, loss_type='L1',\n",
    "                                              logs_folder='./DNN_result/synthetic/training_results_POD', \n",
    "                                              params=params_POD, batch_size=50)\n",
    "print(\"#################################\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b443bb70",
   "metadata": {},
   "source": [
    "## Neural network prediction\n",
    "\n",
    "After the training is finished the best weights are saved for network prediction. Here those weights are loaded and the prediction is performed. The dictionary $test$ is defined here which determines whether to run a multi-query scenario or full prediction scenario. If $test['typeOfTest'] = \"query\"$ then the multi-query scenario is run for which $test['typeOfTest'] = 200$ sets the time step at which the prediction has to be performed. \n",
    "\n",
    "Plotting function is only activated for $test['typeOfTest'] = \"full\"$ which gives us the full prediction throughout all the time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286d7839",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\n",
    "    'typeOfTest': \"full\",\n",
    "    'test_sample': 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6f7ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the model\n",
    "from DFNN import scale_params\n",
    "import torch\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "log_folder_base_sPOD = 'DNN_result/synthetic/training_results_sPOD/'\n",
    "log_folder_trained_model_sPOD = sorted(pathlib.Path(log_folder_base_sPOD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "PATH_sPOD = str(log_folder_trained_model_sPOD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "log_folder_base_POD = 'DNN_result/synthetic/training_results_POD/'\n",
    "log_folder_trained_model_POD = sorted(pathlib.Path(log_folder_base_POD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "PATH_POD = str(log_folder_trained_model_POD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "PATH_sPOD = 'DNN_result/synthetic/training_results_sPOD/2023_02_10__15-10-53/trained_weights/weights.pt'\n",
    "PATH_POD = 'DNN_result/synthetic/training_results_POD/2023_02_10__15-20-51/trained_weights/weights.pt'\n",
    "\n",
    "# Scale the parameters before prediction\n",
    "if '/trained_weights/weights.pt' in PATH_sPOD: address_sPOD = PATH_sPOD.replace('/trained_weights/weights.pt', '')\n",
    "scaling_sPOD = np.load(address_sPOD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "if '/trained_weights/weights.pt' in PATH_POD: address_POD = PATH_POD.replace('/trained_weights/weights.pt', '')\n",
    "scaling_POD = np.load(address_POD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "PARAMS_TEST_sPOD = scale_params(PARAMS_TEST, params_sPOD, scaling_sPOD)\n",
    "PARAMS_TEST_POD = scale_params(PARAMS_TEST, params_POD, scaling_POD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac2a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test['typeOfTest'] == \"query\":\n",
    "    test_sample = test['test_sample']\n",
    "    \n",
    "    ta_test = ta_test[:, test_sample][..., np.newaxis]\n",
    "    \n",
    "    df.TA_TEST = df.TA_TEST[:, test_sample][..., np.newaxis]\n",
    "    df.TA_POD_TEST = df.TA_POD_TEST[:, test_sample][..., np.newaxis]\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(df.NumFrames):\n",
    "        tt = []\n",
    "        for m in range(df.nmodes):\n",
    "            ampl = df.TA_interp_list[i][m][test_sample, :][np.newaxis, ...]\n",
    "            tt.append(ampl)\n",
    "        tmp.append(tt)\n",
    "    df.TA_interp_list = tmp\n",
    "    \n",
    "    df.SHIFTS_TEST[0] = df.SHIFTS_TEST[0][:, test_sample]\n",
    "    df.SHIFTS_TEST[1] = df.SHIFTS_TEST[1][:, test_sample]\n",
    "    \n",
    "    PARAMS_TEST_sPOD = PARAMS_TEST_sPOD[:, test_sample][..., np.newaxis]\n",
    "    PARAMS_TEST_POD = PARAMS_TEST_POD[:, test_sample][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2399d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "from DFNN import test_model \n",
    "rel_err_sPOD, results_predicted_sPOD = test_model(ta_test, PARAMS_TEST_sPOD, saved_model=True, \n",
    "                                                  PATH_TO_WEIGHTS=PATH_sPOD, params=params_sPOD,\n",
    "                                                  scaling=scaling_sPOD, batch_size=50) \n",
    "rel_err_POD, results_predicted_POD = test_model(TA_POD_TEST, PARAMS_TEST_POD, saved_model=True,\n",
    "                                               PATH_TO_WEIGHTS=PATH_POD, params=params_POD,\n",
    "                                               scaling=scaling_POD, batch_size=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0961ee85",
   "metadata": {},
   "source": [
    "Once the predictions for the time amplitudes and the shifts had been made we now reconstruct the snapshot according to Eq.(12) and Eq.(20) for POD and sPOD based methods respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd0dc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "TA_sPOD_pred = results_predicted_sPOD[:-2, :]\n",
    "shifts_sPOD_pred = results_predicted_sPOD[-2:, :]\n",
    "TA_POD_pred = results_predicted_POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204744d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "errors = df.OnlinePredictionAnalysis(TA_sPOD_pred, shifts_sPOD_pred, TA_POD_pred, plot_online=False, test_type=test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
