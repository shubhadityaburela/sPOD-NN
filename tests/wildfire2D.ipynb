{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametric non-intrusive model order reduction for 2D wildland fire model (without wind)\n",
    "\n",
    "Here we describe the application of methods sPOD-NN, sPOD-I and POD-NN on a 2D wildland fire model without any topological changes. The model equations are given in Eq.(30) and Eq.(31) of the paper. The wind velocity is considered to be (0, 0). The results generated in this script are shown in Sec.(3.1.2) of the paper as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../sPOD/lib/')\n",
    "sys.path.append('../DL-ROM/LIB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildfire2D_sup import wildfire2D_sup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basis reconstruction for the 2D wildland fire model\n",
    "\n",
    "In this part we : \n",
    "\n",
    "<ul>\n",
    "<li>Load the 2D wildland fire data.</li>\n",
    "<li>Perform sPOD and POD on the generated data.</li>\n",
    "<li>Extract the time amplitudes according to Eq.(8) and Eq.(13) from the paper.</li>\n",
    "</ul>\n",
    "\n",
    "The inputs here include : \n",
    "<ul>\n",
    "<li>$variable$ which is to be set as 0 for \"Temperature\" and 1 for \"Supply mass fraction\".</li>\n",
    "<li>$test\\_val$ is the parameter value at which the testing is performed.</li>\n",
    "</ul>\n",
    "\n",
    "The sPOD algorithm has to run only once for the basis reconstruction and the results necessary for subsequent steps are stored to save repeated time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable = 0\n",
    "test_val = 558.49\n",
    "\n",
    "if variable == 0:\n",
    "    name = \"T\"\n",
    "else:\n",
    "    name = \"S\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "q = np.load(os.path.abspath(\".\") + '/wildfire_data/2D/' + 'SnapShotMatrix' + str(test_val) + '.npy')\n",
    "shifts_test = np.load(os.path.abspath(\".\") + '/wildfire_data/2D/' + 'Shifts' + str(test_val) + '.npy')\n",
    "\n",
    "df = wildfire2D_sup(q, shifts_test, param_test_val=test_val, var=variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sPOD to be performed only once and the results to be stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################# Run shifted POD on the data ########################## (only once)\n",
    "impath = \"./wildfire_data/2D/save_Wildfire/\" + name + \"/\"\n",
    "import os\n",
    "import pickle\n",
    "os.makedirs(impath, exist_ok=True)\n",
    "\n",
    "U_list, TA_list_training, TA_list_interp, spod_modes = df.run_sPOD(spod_iter=2000)\n",
    "\n",
    "with open(impath + 'U_list.data', 'wb') as filehandle:\n",
    "    pickle.dump(U_list, filehandle)\n",
    "with open(impath + 'TA_list_training.data', 'wb') as filehandle:\n",
    "    pickle.dump(TA_list_training, filehandle)\n",
    "with open(impath + 'TA_list_interp.data', 'wb') as filehandle:\n",
    "    pickle.dump(TA_list_interp, filehandle)\n",
    "with open(impath + 'spod_modes.data', 'wb') as filehandle:\n",
    "    pickle.dump(spod_modes, filehandle)\n",
    "with open(impath + 'Q_polar_train.data', 'wb') as filehandle:\n",
    "    pickle.dump(df.q_polar_train, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = \"./wildfire_data/2D/save_Wildfire/\" + name + \"/\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(impath + 'U_list.data', 'rb') as filehandle:\n",
    "    U_list = pickle.load(filehandle) \n",
    "with open(impath + 'TA_list_training.data', 'rb') as filehandle:\n",
    "    TA_list_training = pickle.load(filehandle)  \n",
    "with open(impath + 'TA_list_interp.data', 'rb') as filehandle:\n",
    "    TA_list_interp = pickle.load(filehandle) \n",
    "with open(impath + 'spod_modes.data', 'rb') as filehandle:\n",
    "    spod_modes = pickle.load(filehandle) \n",
    "with open(impath + 'Q_polar_train.data', 'rb') as filehandle:\n",
    "    Q_polar_train = pickle.load(filehandle) \n",
    "    \n",
    "TA_TRAIN = np.concatenate(TA_list_training, axis=0)\n",
    "SHIFTS_TRAIN = df.shifts_train[0][0]\n",
    "PARAMS_TRAIN = df.params_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "u, s, vt = randomized_svd(np.concatenate(df.q_train, axis=1), n_components=sum(spod_modes) + 1, random_state=None)\n",
    "U_POD_TRAIN = u\n",
    "TA_POD_TRAIN = np.diag(s) @ vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the testing data is assembled. The sPOD and POD are performed on the testing data and then time amplitudes and shifts are extracted. These are used for final error calculations. The sPOD on the testing data is also performed only once for saving time and effort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################# Run shifted POD on the test data ########################## (only once)\n",
    "import os\n",
    "impath = \"./wildfire_data/2D/save_Wildfire/\" + name + \"/\" + str(test_val) + \"/\"\n",
    "import pickle\n",
    "os.makedirs(impath, exist_ok=True)\n",
    "\n",
    "Q_frames_test_polar, Q_frames_test_cart, conv_param = df.test_data(spod_iter=1000)\n",
    "\n",
    "\n",
    "with open(impath + 'Q_frames_test_polar.data', 'wb') as filehandle:\n",
    "    pickle.dump(Q_frames_test_polar, filehandle)\n",
    "with open(impath + 'Q_frames_test_cart.data', 'wb') as filehandle:\n",
    "    pickle.dump(Q_frames_test_cart, filehandle)\n",
    "with open(impath + 'Q_test_polar.data', 'wb') as filehandle:\n",
    "    pickle.dump(df.q_polar_test, filehandle)\n",
    "with open(impath + 'conv_param.data', 'wb') as filehandle:\n",
    "    pickle.dump(conv_param, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = \"./wildfire_data/2D/save_Wildfire/\" + name + \"/\" + str(test_val) + \"/\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(impath + 'Q_frames_test_polar.data', 'rb') as filehandle:\n",
    "    Q_frames_test_polar = pickle.load(filehandle) \n",
    "with open(impath + 'Q_frames_test_cart.data', 'rb') as filehandle:\n",
    "    Q_frames_test_cart = pickle.load(filehandle) \n",
    "with open(impath + 'Q_test_polar.data', 'rb') as filehandle:\n",
    "    Q_test_polar = pickle.load(filehandle) \n",
    "with open(impath + 'conv_param.data', 'rb') as filehandle:\n",
    "    conv_param = pickle.load(filehandle)\n",
    "    \n",
    "# Plot the frames for test parameter\n",
    "df.plot_sPOD_frames(Q_frames_test_cart, plot_every=10, var_name=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_vecs_test = np.asarray([df.param_test_val])\n",
    "params_test = [np.squeeze(np.asarray([[np.ones_like(df.t) * mu], [df.t]])) for mu in mu_vecs_test]\n",
    "PARAMS_TEST = np.concatenate(params_test, axis=1)\n",
    "\n",
    "q1_test = Q_frames_test_polar[0]\n",
    "q2_test = Q_frames_test_polar[1]\n",
    "time_amplitudes_1_test = U_list[0].transpose() @ q1_test\n",
    "time_amplitudes_2_test = U_list[1].transpose() @ q2_test\n",
    "\n",
    "TA_TEST = np.concatenate((time_amplitudes_1_test, time_amplitudes_2_test), axis=0)\n",
    "SHIFTS_TEST = df.shifts_test[0][0]\n",
    "TA_POD_TEST = U_POD_TRAIN.transpose() @ df.q_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assemble the $\\hat{A}$ matrix according to the Eq.(16) from the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_train = np.reshape(SHIFTS_TRAIN, newshape=[1, -1])\n",
    "shifts_test = np.reshape(SHIFTS_TEST, newshape=[1, -1])\n",
    "\n",
    "ta_train = np.concatenate((TA_TRAIN, shifts_train), axis=0)\n",
    "ta_test = np.concatenate((TA_TEST, shifts_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid, Nx : {}, Ny : {}, Nt : {}\".format(len(df.x), len(df.y), len(df.t)))\n",
    "print(\"Number of sPOD frames : {}\".format(len(spod_modes)))\n",
    "print(\"Number of modes (frame wise) : {}, {}\".format(spod_modes[0], spod_modes[1]))\n",
    "print(\"Size of training matrix : {} x {}\".format(int(ta_train.shape[0]), int(ta_train.shape[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network training\n",
    "\n",
    "Based on the data which we obtain from the previous step we train our neural network. For the training we first define certain parameters needed for training step. The parameters are mentioned here are:\n",
    "\n",
    "<ul>\n",
    "<li>$scaling$ activates the min-max data scaling for efficient training.</li>\n",
    "<li>$full\\_order\\_model\\_dimension$ is effectively $M$ which is the total number of grid points.</li>\n",
    "<li>$reduced\\_order\\_model\\_dimension$ is $n_{\\mathrm{dof}}$ mentioned in Eq.(19) in the paper.</li>\n",
    "<li>$totalModes$ is the total number of modes.</li>\n",
    "<li>$num\\_early\\_stop$ defines the early stopping criteria for training step.</li>\n",
    "</ul>\n",
    "\n",
    "Subsequently the hyperparameters are:\n",
    "<ul>\n",
    "<li>$epochs$ sets the total number of epochs for training.</li>\n",
    "<li>$lr$ sets the learning rate for training.</li>\n",
    "<li>$loss\\_type$ is the type of loss to consider while training options being $L1$ or $MSE$.</li>\n",
    "<li>$batch\\_size$ sets the total number of minibatches for the training data to be broken down into for effective training.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_sPOD = {\n",
    "        'scaling': True,\n",
    "        'full_order_model_dimension': len(df.x) * len(df.y), \n",
    "        'reduced_order_model_dimension': ta_train.shape[0],\n",
    "        'totalModes': ta_train.shape[0] - len(spod_modes) + 1, \n",
    "        'num_early_stop': 4000 \n",
    "    }\n",
    "params_POD = {\n",
    "        'scaling': True, \n",
    "        'full_order_model_dimension': len(df.x) * len(df.y), \n",
    "        'reduced_order_model_dimension': TA_POD_TRAIN.shape[0], \n",
    "        'totalModes': TA_POD_TRAIN.shape[0], \n",
    "        'num_early_stop': 4000 \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training the model\n",
    "from DFNN import run_model \n",
    "import time\n",
    "tic_sPOD = time.process_time() \n",
    "print(\"#################################\")\n",
    "print(\"sPOD-NN\")\n",
    "model_sPOD, _, scaling_sPOD = run_model(ta_train, PARAMS_TRAIN, epochs=200000, lr=0.005, loss_type='L1', \n",
    "                                        logs_folder='./DNN_result/wildfire2D/training_results_sPOD/' + name, \n",
    "                                        params=params_sPOD, batch_size=50)\n",
    "print(\"#################################\\n\")\n",
    "toc_sPOD = time.process_time()\n",
    "\n",
    "tic_POD = time.process_time()\n",
    "print(\"#################################\")\n",
    "print(\"POD-NN\")\n",
    "model_POD, _, scaling_POD = run_model(TA_POD_TRAIN, PARAMS_TRAIN, epochs=200000, lr=0.005, loss_type='L1', \n",
    "                                      logs_folder='./DNN_result/wildfire2D/training_results_POD/' + name, \n",
    "                                      params=params_POD, batch_size=50)\n",
    "print(\"#################################\\n\")\n",
    "toc_POD = time.process_time()\n",
    "\n",
    "print(f\"Time consumption in training (sPOD-NN) : {toc_sPOD - tic_sPOD:0.4f} seconds\")\n",
    "print(f\"Time consumption in training (POD-NN) : {toc_POD - tic_POD:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network prediction\n",
    "\n",
    "After the training is finished the best weights are saved for network prediction. Here those weights are loaded and the prediction is performed. The dictionary $test$ is defined here which determines whether to run a multi-query scenario or full prediction scenario. If $test['typeOfTest'] = \"query\"$ then the multi-query scenario is run for which $test['typeOfTest'] = 40$ sets the time step at which the prediction has to be performed. \n",
    "\n",
    "Plotting function is only activated for $test['typeOfTest'] = \"full\"$ which gives us the full prediction throughout all the time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {\n",
    "    'typeOfTest': \"full\",\n",
    "    'test_sample': 40\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pathlib\n",
    "import os\n",
    "from DFNN import scale_params\n",
    "\n",
    "# Load the correct model\n",
    "log_folder_base_sPOD = 'DNN_result/wildfire2D/training_results_sPOD/' + name + '/'\n",
    "log_folder_trained_model_sPOD = sorted(pathlib.Path(log_folder_base_sPOD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "PATH_sPOD = str(log_folder_trained_model_sPOD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "log_folder_base_POD = 'DNN_result/wildfire2D/training_results_POD/' + name + '/'\n",
    "log_folder_trained_model_POD = sorted(pathlib.Path(log_folder_base_POD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "PATH_POD = str(log_folder_trained_model_POD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "PATH_sPOD = 'DNN_result/wildfire2D/training_results_sPOD/' + name + '/2023_03_09__10-35-48/trained_weights/weights.pt'\n",
    "PATH_POD = 'DNN_result/wildfire2D/training_results_POD/' + name + '/2023_03_09__10-44-59/trained_weights/weights.pt'\n",
    "\n",
    "# Scale the parameters before prediction\n",
    "if '/trained_weights/weights.pt' in PATH_sPOD: address_sPOD = PATH_sPOD.replace('/trained_weights/weights.pt', '')\n",
    "scaling_sPOD = np.load(address_sPOD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "if '/trained_weights/weights.pt' in PATH_POD: address_POD = PATH_POD.replace('/trained_weights/weights.pt', '')\n",
    "scaling_POD = np.load(address_POD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "PARAMS_TEST_sPOD = scale_params(PARAMS_TEST, params_sPOD, scaling_sPOD)\n",
    "PARAMS_TEST_POD = scale_params(PARAMS_TEST, params_POD, scaling_POD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test['typeOfTest'] == \"query\":\n",
    "    test_sample = test['test_sample']\n",
    "    \n",
    "    ta_test = ta_test[:, test_sample][..., np.newaxis]\n",
    "    \n",
    "    TA_TEST = TA_TEST[:, test_sample][..., np.newaxis]\n",
    "    TA_POD_TEST = TA_POD_TEST[:, test_sample][..., np.newaxis]\n",
    "    \n",
    "    tmp = []\n",
    "    for i in range(df.NumFrames):\n",
    "        tt = []\n",
    "        for m in range(spod_modes[i]):\n",
    "            ampl = TA_list_interp[i][m][test_sample, :][np.newaxis, ...]\n",
    "            tt.append(ampl)\n",
    "        tmp.append(tt)\n",
    "    TA_list_interp = tmp\n",
    "    \n",
    "    SHIFTS_TEST = SHIFTS_TEST[test_sample]\n",
    "    \n",
    "    PARAMS_TEST_sPOD = PARAMS_TEST_sPOD[:, test_sample][..., np.newaxis]\n",
    "    PARAMS_TEST_POD = PARAMS_TEST_POD[:, test_sample][..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the model\n",
    "from DFNN import test_model \n",
    "import time \n",
    "\n",
    "tic = time.process_time()\n",
    "rel_err_sPOD, results_predicted_sPOD = test_model(ta_test, PARAMS_TEST_sPOD, trained_model=None, saved_model=True, \n",
    "                                                  PATH_TO_WEIGHTS=PATH_sPOD, params=params_sPOD, scaling=scaling_sPOD, \n",
    "                                                  batch_size=50) \n",
    "toc = time.process_time()\n",
    "print(f\"Time consumption in testing sPOD-NN model : {toc - tic:0.4f} seconds\")\n",
    "\n",
    "tic = time.process_time()\n",
    "rel_err_POD, results_predicted_POD = test_model(TA_POD_TEST, PARAMS_TEST_POD, trained_model=None, saved_model=True, \n",
    "                                                PATH_TO_WEIGHTS=PATH_POD, params=params_POD, scaling=scaling_POD, \n",
    "                                                batch_size=50)\n",
    "toc = time.process_time()\n",
    "print(f\"Time consumption in testing POD-NN model : {toc - tic:0.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the predictions for the time amplitudes and the shifts had been made we now reconstruct the snapshot according to Eq.(11) and Eq.(18) for POD and sPOD based methods respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_amplitudes_predicted_sPOD = results_predicted_sPOD[:-1, :]\n",
    "shifts_predicted_sPOD = results_predicted_sPOD[-1:, :]\n",
    "frame_amplitudes_predicted_POD = results_predicted_POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Q_recon_sPOD_cart, Q_recon_POD_cart, Q_recon_interp_cart, errors = df.plot_online_data(frame_amplitudes_predicted_sPOD, \n",
    "                                                                                       frame_amplitudes_predicted_POD, \n",
    "                                                                                       TA_TEST, TA_POD_TEST, TA_list_interp,\n",
    "                                                                                       shifts_predicted_sPOD, SHIFTS_TEST, \n",
    "                                                                                       spod_modes, U_list, U_POD_TRAIN, \n",
    "                                                                                       Q_test_polar, Q_frames_test_polar,\n",
    "                                                                                       conv_param, plot_online=True, \n",
    "                                                                                       test_type=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test['typeOfTest'] != \"query\":\n",
    "    df.plot_recon(Q_recon_sPOD_cart, Q_recon_POD_cart, Q_recon_interp_cart, t_a=10, t_b=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction error plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from Helper import save_fig\n",
    "from statistics import mean\n",
    "\n",
    "impath = \"../plots/images_wildfire2D/\"\n",
    "os.makedirs(impath, exist_ok=True) \n",
    "\n",
    "plt.rcParams.update({\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [\"Computer Modern\"]})\n",
    "\n",
    "SMALL_SIZE = 16   # 16\n",
    "MEDIUM_SIZE = 18   # 18\n",
    "BIGGER_SIZE = 20   # 20\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_modes = np.array([2 + 1, 7 + 1, 9 + 1, 13 + 1, 20 + 1, 24 + 1])\n",
    "E_sPOD_NN = np.array([0.20504, 0.02914, 0.02397, 0.02638, 0.02279, 0.02942])\n",
    "E_sPOD_I = np.array([0.20452, 0.03269, 0.03014, 0.02903, 0.02867, 0.02865])\n",
    "E_POD_NN = np.array([0.69972, 0.37205, 0.30384, 0.21423, 0.13170, 0.11663])\n",
    "\n",
    "err = errors[0]\n",
    "err_max = [max(x) for x in err]\n",
    "err_min = [min(x) for x in err]\n",
    "err_mean = [mean(x) for x in err]\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axs[0].semilogy(truncated_modes, E_sPOD_NN, color=\"red\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{sPOD-NN}}_{\\mathrm{tot}}$\")\n",
    "axs[0].semilogy(truncated_modes, E_sPOD_I, color=\"blue\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{sPOD-I}}_{\\mathrm{tot}}$\")\n",
    "axs[0].semilogy(truncated_modes, E_POD_NN, color=\"black\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{POD-NN}}_{\\mathrm{tot}}$\")\n",
    "axs[0].set_xlabel(r\"$n_{\\mathrm{dof}}$\")\n",
    "axs[0].set_ylabel('Errors')\n",
    "axs[0].grid()\n",
    "axs[0].legend(loc='upper right')\n",
    "\n",
    "axs[1].semilogy(df.t, err_max, color=\"teal\", linestyle='--', label=r\"max$(E^{\\mathrm{sPOD-NN}}_j)$\")\n",
    "axs[1].semilogy(df.t, err_mean, color=\"orange\", linestyle='--', label=r\"mean$(E^{\\mathrm{sPOD-NN}}_j)$\")\n",
    "axs[1].semilogy(df.t, err_min, color=\"dimgrey\", linestyle='--', label=r\"min$(E^{\\mathrm{sPOD-NN}}_j)$\")\n",
    "axs[1].set_xlabel(r\"time $t$\")\n",
    "axs[1].grid()\n",
    "axs[1].legend(loc='center right')\n",
    "\n",
    "save_fig(filepath=impath + 'Rel_err_2D', figure=fig)\n",
    "fig.savefig(impath + \"Rel_err_2D\" + \".pdf\", format='pdf',dpi=200, transparent=True, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
