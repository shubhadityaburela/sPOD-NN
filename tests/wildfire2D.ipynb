{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network prediction for the sPOD-DL-ROM for WIldfire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../DL/')\n",
    "sys.path.append('../sPOD/lib/')\n",
    "sys.path.append('../DL-ROM/LIB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildfire2D import wildfire2D\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data generation / Shifted POD of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "q = np.load(os.path.abspath(\".\") + '/wildfire_data/2D/' + 'SnapShotMatrix558_49.npy')\n",
    "shifts_test= np.load(os.path.abspath(\".\") + '/wildfire_data/2D/' + 'Shifts558_49.npy')\n",
    "\n",
    "df = wildfire2D(q, shifts_test, param_test_val=558.49, var=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################# Run shifted POD on the data ########################## (only once)\n",
    "impath = \"./wildfire_data/2D/save_Wildfire/T/\"\n",
    "import os\n",
    "import pickle\n",
    "os.makedirs(impath, exist_ok=True)\n",
    "\n",
    "U_list, TA_list_training, TA_list_interp, spod_modes = df.run_sPOD(spod_iter=2000)\n",
    "\n",
    "with open(impath + 'U_list.data', 'wb') as filehandle:\n",
    "    pickle.dump(U_list, filehandle)\n",
    "with open(impath + 'TA_list_training.data', 'wb') as filehandle:\n",
    "    pickle.dump(TA_list_training, filehandle)\n",
    "with open(impath + 'TA_list_interp.data', 'wb') as filehandle:\n",
    "    pickle.dump(TA_list_interp, filehandle)\n",
    "with open(impath + 'spod_modes.data', 'wb') as filehandle:\n",
    "    pickle.dump(spod_modes, filehandle)\n",
    "with open(impath + 'Q_polar_train.data', 'wb') as filehandle:\n",
    "    pickle.dump(df.q_polar_train, filehandle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data for NN predictions and analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = \"./wildfire_data/2D/save_Wildfire/T/\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(impath + 'U_list.data', 'rb') as filehandle:\n",
    "    U_list = pickle.load(filehandle) \n",
    "with open(impath + 'TA_list_training.data', 'rb') as filehandle:\n",
    "    TA_list_training = pickle.load(filehandle)  \n",
    "with open(impath + 'TA_list_interp.data', 'rb') as filehandle:\n",
    "    TA_list_interp = pickle.load(filehandle) \n",
    "with open(impath + 'spod_modes.data', 'rb') as filehandle:\n",
    "    spod_modes = pickle.load(filehandle) \n",
    "with open(impath + 'Q_polar_train.data', 'rb') as filehandle:\n",
    "    Q_polar_train = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the offline variables\n",
    "df.plot_offline_data(TA_list_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "TA_TRAIN = np.concatenate(TA_list_training, axis=0)\n",
    "SHIFTS_TRAIN = df.shifts_train[0][0]\n",
    "PARAMS_TRAIN = df.params_train\n",
    "\n",
    "u, s, vt = randomized_svd(Q_polar_train, n_components=sum(spod_modes) + 1, random_state=None)\n",
    "U_POD_TRAIN = u\n",
    "TA_POD_TRAIN = np.diag(s) @ vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################### Temporary cell for dof study ############################################\n",
    "frame_wise_sPOD = [19, 5]\n",
    "Nmf = spod_modes\n",
    "time_amplitudes_1 = TA_TRAIN[:Nmf[0], :]\n",
    "time_amplitudes_2 = TA_TRAIN[Nmf[0]:, :]\n",
    "frame_amplitudes_list_training = [\n",
    "    time_amplitudes_1[:frame_wise_sPOD[0], :],\n",
    "    time_amplitudes_2[:frame_wise_sPOD[1], :]\n",
    "]\n",
    "\n",
    "TA_TRAIN = np.concatenate(frame_amplitudes_list_training, axis=0)\n",
    "U_list = [\n",
    "    U_list[0][:, :frame_wise_sPOD[0]], \n",
    "    U_list[1][:, :frame_wise_sPOD[1]]\n",
    "]\n",
    "spod_modes = frame_wise_sPOD\n",
    "\n",
    "frame_amplitudes_list_interp = []\n",
    "for frame in range(2):\n",
    "    Nmodes = spod_modes[frame]\n",
    "    VT = frame_amplitudes_list_training[frame]\n",
    "    amplitudes = [np.reshape(VT[n, :], [df.Nsamples_train, len(df.t)]).T for n in range(Nmodes)]\n",
    "    frame_amplitudes_list_interp.append(amplitudes)\n",
    "\n",
    "TA_list_interp = frame_amplitudes_list_interp\n",
    "U_POD_TRAIN = U_POD_TRAIN[:, :sum(spod_modes) + 1]\n",
    "TA_POD_TRAIN = TA_POD_TRAIN[:sum(spod_modes) + 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Helper import *\n",
    "############################################\n",
    "X_new = df.X - df.x_c  # Shift the origin to the center of the image\n",
    "Y_new = df.Y - df.y_c\n",
    "r = np.sqrt(X_new ** 2 + Y_new ** 2).flatten()  # polar coordinate r\n",
    "theta = np.arctan2(Y_new, X_new).flatten()  # polar coordinate theta\n",
    "r_i = np.linspace(np.min(r), np.max(r), df.Nx)\n",
    "theta_i = np.linspace(np.min(theta), np.max(theta), df.Ny)\n",
    "dr = r_i[1] - r_i[0]\n",
    "dtheta = theta_i[1] - theta_i[0]\n",
    "d_del = np.asarray([dr, dtheta])\n",
    "L = np.asarray([r_i[-1], theta_i[-1]])\n",
    "data_shape = [df.Nx, df.Ny, 1, df.Nsamples_train*df.Nt]\n",
    "Ndims = 2\n",
    "\n",
    "q1 = U_list[0] @ frame_amplitudes_list_training[0] \n",
    "q2 = U_list[1] @ frame_amplitudes_list_training[1]\n",
    "q_train = [np.reshape(q1, newshape=data_shape), np.reshape(q2, newshape=data_shape)]\n",
    "\n",
    "trafos = [\n",
    "transforms(data_shape, L, shifts=df.shifts_train[0], dx=d_del,\n",
    "                          use_scipy_transform=True),\n",
    "transforms(data_shape, L, shifts=df.shifts_train[1], trafo_type=\"identity\",\n",
    "                          dx=d_del,\n",
    "                          use_scipy_transform=True)\n",
    "]\n",
    "\n",
    "NumFrames = 2\n",
    "q_sPOD = 0\n",
    "for frame in range(NumFrames):\n",
    "    q_sPOD += trafos[frame].apply(q_train[frame])\n",
    "############################################\n",
    "q_POD = U_POD_TRAIN @ TA_POD_TRAIN\n",
    "q_POD = np.reshape(q_POD, newshape=data_shape)\n",
    "############################################\n",
    "q_original = np.reshape(Q_polar_train, newshape=data_shape)\n",
    "############################################\n",
    "\n",
    "res = q_original - q_sPOD\n",
    "err_sPOD = np.linalg.norm(np.reshape(res, -1)) / np.linalg.norm(np.reshape(q_original, -1))\n",
    "\n",
    "res = q_original - q_POD\n",
    "err_POD = np.linalg.norm(np.reshape(res, -1)) / np.linalg.norm(np.reshape(q_original, -1))\n",
    "\n",
    "print(\"Relative reconstruction error indicator for full snapshot(sPOD) is {}\".format(err_sPOD))\n",
    "print(\"Relative reconstruction error indicator for full snapshot(POD) is {}\".format(err_POD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "############################# Run shifted POD on the test data ########################## (only once)\n",
    "import os\n",
    "impath = \"./wildfire_data/2D/save_Wildfire/T/\"\n",
    "import pickle\n",
    "os.makedirs(impath, exist_ok=True)\n",
    "\n",
    "Q_frames_test_polar, Q_frames_test_cart = df.test_data(spod_iter=1000)\n",
    "\n",
    "with open(impath + 'Q_frames_test_polar.data', 'wb') as filehandle:\n",
    "    pickle.dump(Q_frames_test_polar, filehandle)\n",
    "with open(impath + 'Q_frames_test_cart.data', 'wb') as filehandle:\n",
    "    pickle.dump(Q_frames_test_cart, filehandle)\n",
    "with open(impath + 'Q_test_polar.data', 'wb') as filehandle:\n",
    "    pickle.dump(df.q_polar_test, filehandle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impath = \"./wildfire_data/2D/save_Wildfire/T/\"\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "with open(impath + 'Q_frames_test_polar.data', 'rb') as filehandle:\n",
    "    Q_frames_test_polar = pickle.load(filehandle) \n",
    "with open(impath + 'Q_frames_test_cart.data', 'rb') as filehandle:\n",
    "    Q_frames_test_cart = pickle.load(filehandle) \n",
    "with open(impath + 'Q_test_polar.data', 'rb') as filehandle:\n",
    "    Q_test_polar = pickle.load(filehandle) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the frames for test parameter\n",
    "df.plot_sPOD_frames(Q_frames_test_cart, plot_every=10, var_name=\"T\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_vecs_test = np.asarray([df.param_test_val])\n",
    "params_test = [np.squeeze(np.asarray([[np.ones_like(df.t) * mu], [df.t]])) for mu in mu_vecs_test]\n",
    "PARAMS_TEST = np.concatenate(params_test, axis=1)\n",
    "\n",
    "q1_test = Q_frames_test_polar[0]\n",
    "q2_test = Q_frames_test_polar[1]\n",
    "time_amplitudes_1_test = U_list[0].transpose() @ q1_test\n",
    "time_amplitudes_2_test = U_list[1].transpose() @ q2_test\n",
    "\n",
    "TA_TEST = np.concatenate((time_amplitudes_1_test, time_amplitudes_2_test), axis=0)\n",
    "SHIFTS_TEST = df.shifts_test[0][0]\n",
    "TA_POD_TEST = U_POD_TRAIN.transpose() @ Q_test_polar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Grid, Nx : {}, Ny : {}, Nt : {}\".format(len(df.x), len(df.y), len(df.t)))\n",
    "print(\"Number of sPOD frames : {}\".format(len(spod_modes)))\n",
    "print(\"Number of modes (frame wise) : {}, {}\".format(spod_modes[0], spod_modes[1]))\n",
    "print(\"Size of training matrix : {} x {}\".format(int(TA_TRAIN.shape[0]), int(TA_TRAIN.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shifts_train = np.reshape(SHIFTS_TRAIN, newshape=[1, -1])\n",
    "shifts_test = np.reshape(SHIFTS_TEST, newshape=[1, -1])\n",
    "\n",
    "ta_train = np.concatenate((TA_TRAIN, shifts_train), axis=0)\n",
    "ta_test = np.concatenate((TA_TEST, shifts_test), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feed forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_sPOD = {\n",
    "#         'scaling': True,  # true if the data should be scaled\n",
    "#         'full_order_model_dimension': len(df.x) * len(df.y),  # N_h\n",
    "#         'reduced_order_model_dimension': ta_train.shape[0],  # N\n",
    "#         'totalModes': ta_train.shape[0] - len(spod_modes) + 1,  # Total number of modes for all the frames\n",
    "#         'num_early_stop': 3000  # Early stop criteria \n",
    "#     }\n",
    "# params_POD = {\n",
    "#         'scaling': True,  # true if the data should be scaled\n",
    "#         'full_order_model_dimension': len(df.x) * len(df.y),  # N_h\n",
    "#         'reduced_order_model_dimension': TA_POD_TRAIN.shape[0],  # N\n",
    "#         'totalModes': TA_POD_TRAIN.shape[0],  # Total number of modes for all the frames\n",
    "#         'num_early_stop': 3000  # Early stop criteria \n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training the model\n",
    "# from network import run_model \n",
    "# print(\"#################################\")\n",
    "# print(\"sPOD-DL-ROM\")\n",
    "# model_sPOD, scaling_sPOD = run_model(ta_train, PARAMS_TRAIN, epochs=100000, lr=0.05, loss_type='L1', \n",
    "#                        logs_folder='./DNN_result/wildfire2D/training_results_sPOD/T',\n",
    "#                       pretrained_load=False, pretrained_weights=None, params=params_sPOD, batch_size=100)\n",
    "# print(\"#################################\\n\")\n",
    "# print(\"#################################\")\n",
    "# print(\"POD-DL-ROM\")\n",
    "# model_POD, scaling_POD = run_model(TA_POD_TRAIN, PARAMS_TRAIN, epochs=100000, lr=0.05, loss_type='L1', \n",
    "#                       logs_folder='./DNN_result/wildfire2D/training_results_POD/T',\n",
    "#                      pretrained_load=False, pretrained_weights=None, params=params_POD, batch_size=100)\n",
    "# print(\"#################################\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading the model\n",
    "# import torch\n",
    "# import pathlib\n",
    "# import os\n",
    "\n",
    "# log_folder_base_sPOD = 'DNN_result/wildfire2D/training_results_sPOD/T/'\n",
    "# log_folder_trained_model_sPOD = sorted(pathlib.Path(log_folder_base_sPOD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "# PATH_sPOD = str(log_folder_trained_model_sPOD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "\n",
    "# log_folder_base_POD = 'DNN_result/wildfire2D/training_results_POD/T/'\n",
    "# log_folder_trained_model_POD = sorted(pathlib.Path(log_folder_base_POD).glob('*/'), key=os.path.getmtime)[-1]\n",
    "# PATH_POD = str(log_folder_trained_model_POD) + '/trained_weights/' + 'weights.pt'\n",
    "\n",
    "# print(PATH_sPOD)\n",
    "# print(PATH_POD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from network import scale_params\n",
    "\n",
    "# if '/trained_weights/weights.pt' in PATH_sPOD: address_sPOD = PATH_sPOD.replace('/trained_weights/weights.pt', '')\n",
    "# scaling_sPOD = np.load(address_sPOD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "# if '/trained_weights/weights.pt' in PATH_POD: address_POD = PATH_POD.replace('/trained_weights/weights.pt', '')\n",
    "# scaling_POD = np.load(address_POD + '/variables/' + 'scaling.npy', allow_pickle=True)\n",
    "\n",
    "# PARAMS_TEST_sPOD = scale_params(PARAMS_TEST, params_sPOD, scaling_sPOD)\n",
    "# PARAMS_TEST_POD = scale_params(PARAMS_TEST, params_POD, scaling_POD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # testing the model\n",
    "# from network import test_model \n",
    "# import time \n",
    "\n",
    "# tic = time.process_time()\n",
    "# rel_err_sPOD, results_predicted_sPOD = test_model(ta_test, PARAMS_TEST_sPOD, \n",
    "#                                                   trained_model=None, saved_model=True,\n",
    "#                                                  PATH_TO_WEIGHTS=PATH_sPOD, params=params_sPOD, \n",
    "#                                                   scaling=scaling_sPOD, batch_size=500) \n",
    "# toc = time.process_time()\n",
    "# print(f\"Time consumption in testing sPOD DL model : {toc - tic:0.4f} seconds\")\n",
    "\n",
    "# tic = time.process_time()\n",
    "# rel_err_POD, results_predicted_POD = test_model(TA_POD_TEST, PARAMS_TEST_POD, \n",
    "#                                                 trained_model=None, saved_model=True,\n",
    "#                                                PATH_TO_WEIGHTS=PATH_POD, params=params_POD,\n",
    "#                                                scaling=scaling_POD, batch_size=500)\n",
    "# toc = time.process_time()\n",
    "# print(f\"Time consumption in testing POD DL model : {toc - tic:0.4f} seconds\")\n",
    "\n",
    "\n",
    "# print(rel_err_sPOD, rel_err_POD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This cell is reserved for data manipulations for the online analysis\n",
    "# frame_amplitudes_predicted_sPOD = results_predicted_sPOD[:-1, :]\n",
    "# shifts_predicted_sPOD = results_predicted_sPOD[-1:, :]\n",
    "# frame_amplitudes_predicted_POD = results_predicted_POD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Q_recon_sPOD_cart, Q_recon_POD_cart, Q_recon_interp_cart, errors = df.plot_online_data(frame_amplitudes_predicted_sPOD, \n",
    "#                                         frame_amplitudes_predicted_POD,\n",
    "#                                         TA_TEST,\n",
    "#                                         TA_POD_TEST,\n",
    "#                                         TA_list_interp,                          \n",
    "#                                         shifts_predicted_sPOD,\n",
    "#                                         SHIFTS_TEST,\n",
    "#                                         spod_modes,\n",
    "#                                         U_list,\n",
    "#                                         U_POD_TRAIN,\n",
    "#                                         Q_test_polar,\n",
    "#                                         Q_frames_test_polar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.plot_recon(Q_recon_sPOD_cart, Q_recon_POD_cart, Q_recon_interp_cart, t_a=10, t_b=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Erros Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import os\n",
    "# from Helper import save_fig\n",
    "# from statistics import mean\n",
    "\n",
    "# impath = \"../plots/images_wildfire2D/\"\n",
    "# os.makedirs(impath, exist_ok=True) \n",
    "\n",
    "# plt.rcParams.update({\n",
    "#     \"text.usetex\": True,\n",
    "#     \"font.family\": \"serif\",\n",
    "#     \"font.serif\": [\"Computer Modern\"]})\n",
    "\n",
    "# SMALL_SIZE = 16   # 16\n",
    "# MEDIUM_SIZE = 18   # 18\n",
    "# BIGGER_SIZE = 20   # 20\n",
    "\n",
    "# plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "# plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title\n",
    "# plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "# plt.rc('xtick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('ytick', labelsize=MEDIUM_SIZE)    # fontsize of the tick labels\n",
    "# plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "# plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated_modes = np.array([3, 6, 14, 20, 30, 40, 46])\n",
    "# E_sPOD_NN = np.array([0.99486, 0.96774, 0.18548, 0.18436, 0.06719, 0.0554, 0.03354])\n",
    "# E_POD_NN = np.array([0.55695, 0.38588, 0.221744, 0.167523, 0.09577, 0.10325, 0.08427])\n",
    "# E_sPOD_LI = np.array([0.27787, 0.2768, 0.05805, 0.05771, 0.05576, 0.05573, 0.05573])\n",
    "\n",
    "# err = errors[0]\n",
    "# err_min = [max(x) for x in err]\n",
    "# err_max = [min(x) for x in err]\n",
    "# err_mean = [mean(x) for x in err]\n",
    "\n",
    "# fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# axs[0].semilogy(truncated_modes, E_sPOD_NN, color=\"green\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{sPOD-NN}}_{\\mathrm{tot}}$\")\n",
    "# axs[0].semilogy(truncated_modes, E_POD_NN, color=\"blue\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{POD-NN}}_{\\mathrm{tot}}$\")\n",
    "# axs[0].semilogy(truncated_modes, E_sPOD_LI, color=\"yellow\", linestyle='--', marker=\"*\", label=r\"$E^{\\mathrm{sPOD-LI}}_{\\mathrm{tot}}$\")\n",
    "# axs[0].set_xlabel('Number of modes')\n",
    "# axs[0].set_ylabel('Errors')\n",
    "# axs[0].grid()\n",
    "# axs[0].legend(loc='upper right')\n",
    "\n",
    "# axs[1].semilogy(df.t, err_min, color=\"green\", linestyle='--', label=r\"$min(E_{\\mathrm{t}})$\")\n",
    "# axs[1].semilogy(df.t, err_max, color=\"black\", linestyle='--', label=r\"$max(E_{\\mathrm{t}})$\")\n",
    "# axs[1].semilogy(df.t, err_mean, color=\"red\", linestyle='--', label=r\"$mean(E_{\\mathrm{t}})$\")\n",
    "# axs[1].set_xlabel(r\"time $t$\")\n",
    "# axs[1].grid()\n",
    "# axs[1].legend(loc='lower right')\n",
    "\n",
    "\n",
    "# save_fig(filepath=impath + 'Rel_err', figure=fig)\n",
    "# fig.savefig(impath + \"Rel_err\" + \".eps\", format='eps',dpi=600, transparent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
